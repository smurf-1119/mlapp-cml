{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Load the data and split them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16876\n"
     ]
    }
   ],
   "source": [
    "oush_df = pd.read_csv('./reuters_10class.csv')\n",
    "print(len(oush_df))\n",
    "# reuters_df = pd.read_csv('./reuters1000.csv')\n",
    "# reuters_df.to_csv('./reuters1000.csv',index=False)\n",
    "# print('Finish')\n",
    "train_size = int(0.7 * len(oush_df))\n",
    "oush_Y_train = oush_df.iloc[:train_size,1:].values\n",
    "oush_Y_test = oush_df.iloc[train_size:,1:].values\n",
    "# reuters_Y_train = reuters_df.iloc[:train_size,1:5].values\n",
    "# reuters_Y_test = reuters_df.iloc[train_size:,1:5].values\n",
    "# print(reuters_Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cleanse(text):\n",
    "    text = text.lower()\n",
    "    filters = ['\"','#','$','%','&','\\(','\\)','\\*','\\+',',','-','\\.','/',':',';','<','=','>','\\?','@','\\[','\\\\','\\]'\n",
    "               ,'^','_',' ','\\{','\\|','\\}','~','\\t','\\n','\\x97','\\x96']\n",
    "    text = re.sub(\"<.*?>\",' ',text)\n",
    "    text = re.sub(\"|\".join(filters),\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11813, 400) (5063, 9)\n"
     ]
    }
   ],
   "source": [
    "def encoder(dataset,max_feature):\n",
    "    content = dataset.iloc[:,0].values.tolist()\n",
    "    freq=pd.Series(' '.join(content).split()).value_counts()\n",
    "    total_size = len(content)\n",
    "    new_contents = []\n",
    "    document = []\n",
    "    from nltk.stem import PorterStemmer\n",
    "    st = PorterStemmer()\n",
    "    for text in content:\n",
    "        new_text = cleanse(text)\n",
    "        split_content = word_tokenize(new_text)\n",
    "        words = [word for word in split_content if word not in stopwords.words('english')]\n",
    "        words = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "        # words = [st.stem(w) for w in words]\n",
    "        words = [w for w in words if w not in freq[:10]]\n",
    "        \n",
    "        new_contents.append([(' '.join(words))])\n",
    "\n",
    "    # counter = collections.Counter([tk for st in new_contents for tk in st[0].split()])\n",
    "    # counter = list(dict(filter(lambda x: x[1] >= 1, counter.items())).keys())\n",
    "    # contents = []\n",
    "\n",
    "    # for text in new_contents:\n",
    "    #     words = [w for w in text[0].split() if w in counter]\n",
    "    #     if len(words):\n",
    "    #         contents.append([' '.join(words)])\n",
    "    # new_contents = contents\n",
    "\n",
    "    # features = len(counter)\n",
    "    documents = [new_content[0] for new_content in new_contents]\n",
    "    vectorize = CountVectorizer(max_features=400)\n",
    "    vectorize.fit(documents)\n",
    "    features = len(vectorize.vocabulary_)\n",
    "    X_train = np.ones(shape=[train_size,features],dtype=np.int64)\n",
    "    X_test = np.ones(shape=[len(new_contents)-train_size,features])\n",
    "    for idx,new_content in enumerate(new_contents):\n",
    "        word_vector = vectorize.transform(new_content).toarray()\n",
    "        if idx >= train_size: \n",
    "            X_test[idx-train_size] = word_vector\n",
    "        else:\n",
    "             X_train[idx] = word_vector\n",
    "    return X_train,X_test\n",
    "\n",
    "max_feature = 400\n",
    "oush_X_train,oush_X_test = encoder(oush_df,max_feature)\n",
    "print(oush_X_train.shape,oush_Y_test.shape)\n",
    "# reuters_X_train,reuters_X_test = encoder(reuters_df,max_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5063, 400)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oush_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "np.save('./traindataReuters_10class.npy',oush_X_train)\n",
    "np.save('./trainlabelReuters_10class.npy',oush_Y_train)                               \n",
    "np.save('./testdataReuters_10class.npy',oush_X_test)\n",
    "np.save('./testlabelReuters_10class.npy',oush_Y_test)\n",
    "print('Finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
