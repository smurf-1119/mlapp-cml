{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oush_df = pd.read_csv('./new_data.csv')\n",
    "reuters_df = pd.read_csv('./reuters1000.csv', index_col=None).iloc[:,:5]\n",
    "# reuters_df.to_csv('./reuters1000.csv',index=False)\n",
    "# print('Finish')\n",
    "train_size = 700\n",
    "Y_train = oush_df.iloc[:train_size,1:].values\n",
    "Y_test = oush_df.iloc[train_size:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pl_usa</th>\n",
       "      <th>to_earn</th>\n",
       "      <th>to_acq</th>\n",
       "      <th>pl_uk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>showers continued throughout the week in\\nthe ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard oil co and bp north america\\ninc said...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>texas commerce bancshares inc's texas\\ncommerc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  pl_usa  to_earn  to_acq  \\\n",
       "0  showers continued throughout the week in\\nthe ...       1        0       0   \n",
       "1  standard oil co and bp north america\\ninc said...       1        0       0   \n",
       "2  texas commerce bancshares inc's texas\\ncommerc...       1        0       0   \n",
       "\n",
       "   pl_uk  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cleanse(text):\n",
    "    text = text.lower()\n",
    "    filters = ['\"','#','$','%','&','\\(','\\)','\\*','\\+',',','-','\\.','/',':',';','<','=','>','\\?','@','\\[','\\\\','\\]'\n",
    "               ,'^','_',' ','\\{','\\|','\\}','~','\\t','\\n','\\x97','\\x96']\n",
    "    text = re.sub(\"<.*?>\",' ',text)\n",
    "    text = re.sub(\"|\".join(filters),\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = np.array(oush_df.iloc[:3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['evaluation', '99tcm', 'technegas', 'ventilation', 'scintigraphy', 'diagnosis', 'pulmonary', 'embolism', 'diagnosis', 'pulmonary', 'embolism', 'perfusion', 'lung', 'scintigraphy', 'offer', 'high', 'sensitivity', 'low', 'specificity', 'specificity', 'significantly', 'increased', 'use', 'combined', 'ventilation', 'perfusion', 'study', 'aspect', 'perfusion', 'lung', 'scintigraphy', 'uniformly', 'accepted', 'technique', 'ventilation', 'imaging', 'varies', 'centre', 'centre', 'study', 'describes', 'new', 'technique', 'performance', 'ventilation', 'scintigraphy', 'using', 'suspension', 'ultrafine', 'carbon', 'particle', 'labelled', '99tcm', 'technegas', 'technique', 'combine', 'ready', 'availability', '99tcm', 'optimal', 'imaging', 'property', 'easily', 'administered', 'radiopharmaceutical', 'particle', 'size', 'sufficiently', 'small', 'deposit', 'alveolus', '63', 'patient', 'studied', 'conventional', 'perfusion', 'scintigraphy', 'plus', 'technegas', 'ventilation', 'scintigraphy', 'image', 'diagnostic', 'quality', 'obtained', '31', 'patient', 'also', 'ventilation', 'study', 'using', '81krm', 'gas', 'one', 'instance', 'two', 'method', 'ventilation', 'imaging', 'lead', 'differing', 'interpretation', 'conclude', 'high', 'quality', 'diagnostic', 'image', 'may', 'obtained', 'using', 'new', 'technique', 'made', 'available', 'routine', 'emergency', 'basis', 'thus', 'improving', 'service', 'provided', 'patient', 'suspected', 'pulmonary', 'embolism'], ['cardiorespiratory', 'effect', 'pressure', 'controlled', 'ventilation', 'severe', 'respiratory', 'failure', 'cardiorespiratory', 'value', 'measured', 'ten', 'patient', 'severe', 'respiratory', 'failure', 'volume', 'controlled', 'pressure', 'controlled', 'ventilation', 'tidal', 'volume', 'respirator', 'rate', 'peep', 'auto', 'peep', 'inspiratory', 'expiratory', 'ratio', '1', '2', 'fio2', 'maintained', 'value', 'ventilatory', 'modality', 'changing', 'vcv', 'pcv', 'associated', 'significant', 'improvement', 'pao2', 'oxygen', 'delivery', 'tissue', 'oxygen', 'consumption', 'peak', 'inspiratory', 'pressure', 'fell', 'significant', 'change', 'cardiorespiratory', 'value', 'arterial', 'blood', 'pressure', 'ventilatory', 'measurement', 'mean', 'airway', 'pressure', 'associated', 'use', 'pcv', 'result', 'suggest', 'pcv', 'may', 'beneficial', 'ventilatory', 'modality', 'treatment', 'severe', 'respiratory', 'failure', 'since', 'result', 'improvement', 'arterial', 'oxygenation', 'tissue', 'oxygen', 'delivery', 'utilization', 'without', 'concomitant', 'adverse', 'effect', 'hemodynamic', 'ventilatory', 'factor']]\n"
     ]
    }
   ],
   "source": [
    "content = np.array(oush_df.iloc[:,0])\n",
    "total_size = len(content)\n",
    "new_contents = []\n",
    "document = []\n",
    "for text in content:\n",
    "    new_text = cleanse(text)\n",
    "    split_content = word_tokenize(new_text)\n",
    "    words = [word for word in split_content if word not in stopwords.words('english')]\n",
    "    lemmed_word = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "    document.append(' '.join(lemmed_word))\n",
    "    new_contents.append(lemmed_word)\n",
    "\n",
    "print(new_contents[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counter = collections.Counter([word for words in new_contents for word in words ])\n",
    "# 'The count of elements not in the Counter is zero.'\n",
    "        # Needed so that self[missing_item] does not raise KeyError\n",
    "counter = dict(filter(lambda x: x[1] >= 300, counter.items()))\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "value=list(counter.values())\n",
    "list.sort(value, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[:int(0.1*len(value))][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将词映射到整数索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "#            for st in new_contents]\n",
    "dataset = np.zeros(shape=[len(new_contents), len(idx_to_token)])\n",
    "for i in range(len(new_contents)):\n",
    "    for j in range(len(new_contents[i])):\n",
    "        if new_contents[i][j] in token_to_idx:\n",
    "            dataset[i,token_to_idx[new_contents[i][j]]] += 1\n",
    "        # else:\n",
    "        #     dataset[i,-1] += 1\n",
    "# dataset = dataset / np.sum(dataset, axis=1, keepdims=True)\n",
    "        \n",
    "num_tokens = sum([len(st) for st in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 0.05, 0.  , 0.45, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.2 , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.05, 0.1 , 0.05, 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vectorize = CountVectorizer(max_features=300,min_df=0.3)\n",
    "# vectorize.fit(document)\n",
    "# features = len(vectorize.vocabulary_)\n",
    "# X_train = np.ones(shape=[train_size,features],dtype=np.int64)\n",
    "# X_test = np.ones(shape=[total_size-train_size,features])\n",
    "# for idx,new_content in enumerate(new_contents):\n",
    "#     word_vector = vectorize.transform(new_content).toarray()\n",
    "#     if idx >= train_size:\n",
    "#         X_test[idx] = word_vector\n",
    "#     else:\n",
    "#          X_train[idx] = word_vector\n",
    "# np.save('./oush_sample_train.npy',X_train)\n",
    "# np.save('./oush_test.npy',X_test)\n",
    "# np.save('./oush_label_train',Y_train)\n",
    "# np.save('./oush_label_test',Y_test)\n",
    "# print('Finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
