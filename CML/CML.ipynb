{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.special import comb  # 排列组合中的组合公式\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_k(dataSet, Labels, d, q, device):\n",
    "    \"\"\"\n",
    "    :param dataSet: 某一个样本的特征集\n",
    "    :param Labels: 某一个样本的标签集\n",
    "    :param d: 样本的维度，即一个样本含有的特征数\n",
    "    :param q: 标签的维度，即标签集中标签的个数\n",
    "    :return: 返回的是fk(x,y)\n",
    "    \"\"\"\n",
    "    F_k = []\n",
    "    for l in range(d):\n",
    "        for j in range(q):\n",
    "            if Labels[j] == 1:\n",
    "                try:\n",
    "                    F_k.append(dataSet[l])\n",
    "                except:\n",
    "                    # print(dataSet)\n",
    "                    # print(l, dataSet)\n",
    "                    raise IndexError\n",
    "\n",
    "            else:\n",
    "                F_k.append(\n",
    "                    torch.tensor(\n",
    "                        0, dtype=torch.float, requires_grad=True, device=device\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    for j1 in range(q - 1):\n",
    "        for j2 in range(j1 + 1, q):\n",
    "            y_j1 = Labels[j1]\n",
    "            y_j2 = Labels[j2]\n",
    "            if y_j1 == 1 and y_j2 == 1:\n",
    "                F_k.append(\n",
    "                    torch.tensor(\n",
    "                        1, dtype=torch.float, requires_grad=True, device=device\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                F_k.append(\n",
    "                    torch.tensor(\n",
    "                        0, dtype=torch.float, requires_grad=True, device=device\n",
    "                    )\n",
    "                )\n",
    "            if y_j1 == 1 and y_j2 == 0:\n",
    "                F_k.append(\n",
    "                    torch.tensor(\n",
    "                        1, dtype=torch.float, requires_grad=True, device=device\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                F_k.append(\n",
    "                    torch.tensor(\n",
    "                        0, dtype=torch.float, requires_grad=True, device=device\n",
    "                    )\n",
    "                )\n",
    "            if y_j1 == 0 and y_j2 == 1:\n",
    "                F_k.append(\n",
    "                    torch.tensor(\n",
    "                        1, dtype=torch.float, requires_grad=True, device=device\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                F_k.append(\n",
    "                    torch.tensor(\n",
    "                        0, dtype=torch.float, requires_grad=True, device=device\n",
    "                    )\n",
    "                )\n",
    "            if y_j1 == 0 and y_j2 == 0:\n",
    "                F_k.append(\n",
    "                    torch.tensor(\n",
    "                        1, dtype=torch.float, requires_grad=True, device=device\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                F_k.append(\n",
    "                    torch.tensor(\n",
    "                        0, dtype=torch.float, requires_grad=True, device=device\n",
    "                    )\n",
    "                )\n",
    "    # print(len(F_k))\n",
    "    return torch.tensor(F_k, requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_rand_labels(len):\n",
    "    \"\"\"\n",
    "    变成辅助函数\n",
    "    #关于这个函数的for循环的嵌套次数，Y标签集中，有几个标签就嵌套几层。（y1,y2,...,yq）\n",
    "    :return: 返回的是q维的标签集的所有组合情况\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    randLabels=[]\n",
    "    for i in range(2):\n",
    "        randLabels.append([i])\n",
    "    return randLabels\n",
    "    \"\"\"\n",
    "    randLabels = []\n",
    "    for i in range(2 ** len):\n",
    "        randLabel = np.zeros(shape=len)\n",
    "        for j in range(len):\n",
    "            randLabel[len - j - 1] = i % 2\n",
    "            i = i // 2\n",
    "            if i == 0:\n",
    "                break\n",
    "        print(randLabel)\n",
    "        randLabels.append(randLabel)\n",
    "    np.save(\"./basic_rand_Labels.npy\", np.array(randLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supported_rand_labels(train_label):\n",
    "    \"\"\"\n",
    "    这是个辅助函数，用来生成support_rand_Labels\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    randLabels=[]\n",
    "    for i in range(2):\n",
    "        randLabels.append([i])\n",
    "    return randLabels\n",
    "    \"\"\"\n",
    "    # for _, y in train_iter:\n",
    "    labels = train_label.tolist()\n",
    "    label_set = []\n",
    "    for label in labels:\n",
    "        if label in label_set:\n",
    "            continue\n",
    "        else:\n",
    "            label_set.append(label)\n",
    "    randLables = np.array(label_set)\n",
    "    print(label_set)\n",
    "    np.save(\"./supported_rand_labels.npy\", randLables)\n",
    "    print(\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**用于产生randLabels，输入的mode为basic或者是supported**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_rand_Labels(mode):\n",
    "    if mode == \"supported\":\n",
    "        randLabels = np.load(\"./supported_rand_labels.npy\")\n",
    "        randLabels = randLabels.tolist()\n",
    "        return randLabels\n",
    "    elif mode == \"basic\":\n",
    "        randLabels = np.load(\"./basic_rand_Labels.npy\")\n",
    "        randLabels = randLabels.tolist()\n",
    "        return randLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z(dataSet, d, q, Lambda, device, randLabels):  # 对于某一个样本的Z\n",
    "    \"\"\"\n",
    "    :param dataSet: 某一个样本的特征集\n",
    "    :param d: 样本的维度，即特征的个数\n",
    "    :param q: 标签集的个数\n",
    "    :param Lambda: Lambda是一个1*K维向量\n",
    "    :return: 归一化范数，以及所有标签集组合的对应f_k\n",
    "    \"\"\"\n",
    "    Z = torch.tensor(0.0, requires_grad=True, device=device)\n",
    "    for i in range(len(randLabels)):\n",
    "        fk = f_k(dataSet, randLabels[i], d, q, device)\n",
    "        temp_sum = torch.exp((Lambda * fk).sum())\n",
    "        Z = Z + temp_sum\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求目标函数l(Lambda|D)\n",
    "def obj_func(DataSets, Labels, thegma, Lambda, randLabels, device):\n",
    "    \"\"\"\n",
    "    :param q:标签集的维度\n",
    "    :param DataSets:所有训练样本的特征集\n",
    "    :param Labels:所有训练样本的标签集\n",
    "    :param thegma:自己给定的参数值，2**-6,2**-5,2**-4,2**-3,2**-2,2**-1,2**1,2**2,2**3,2**4,2**5,2**6逐个取值，参数寻优\n",
    "    :return:目标函数，以及待定参数Lambda\n",
    "    \"\"\"\n",
    "    samples = len(DataSets)\n",
    "    d = len(DataSets[0])\n",
    "    q = len(Labels[0])\n",
    "    temp_sum = torch.tensor(0.0, requires_grad=True, device=device)\n",
    "    for i in range(samples):\n",
    "        fk = f_k(DataSets[i], Labels[i], d, q, device)\n",
    "        z = Z(DataSets[i], d, q, Lambda, device, randLabels)\n",
    "        temp_sum = temp_sum + (Lambda * fk).sum() - torch.log2(z)\n",
    "        temp_div = (\n",
    "            (Lambda * Lambda) / (2 * thegma ** 2)\n",
    "        ).sum()  # temp_div=sum(Lambda**2/(2*thegma**2))\n",
    "\n",
    "    l = -(temp_sum - temp_div)\n",
    "    return l  # 求解l的最大值，可以转化为求-l的最小值问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(\n",
    "    objfunc,\n",
    "    train_iter,\n",
    "    test_iter,\n",
    "    test_target,\n",
    "    num_epochs,\n",
    "    optimizer,\n",
    "    thegma,\n",
    "    Lambda,\n",
    "    device,\n",
    "    randLabels,\n",
    "):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        Lambda.to(device)\n",
    "        for X, y in tqdm(train_iter, total=len(train_iter), ascii=True, desc=\"train\"):\n",
    "            \n",
    "            # def closure():\n",
    "            #     loss = objfunc(X, y, thegma, Lambda, randLabels, device)\n",
    "            #     optimizer.zero_grad()\n",
    "            #     loss.backward()\n",
    "            #     return loss\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = objfunc(X, y, thegma, Lambda, randLabels, device)\n",
    "            train_l_sum += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # optimizer.step(closure)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        hamming, f1_macro, f1_micro, acc = test(\n",
    "            test_iter, test_target, Lambda, d, q, randLabels\n",
    "        )\n",
    "        print(\n",
    "            \"epoch %d, time %.1f sec, train_loss %.2f, hamming %.2f, f1_macro %.2f, f1_micro %.2f, subset acc %.4f acc\"\n",
    "            % (epoch + 1, time.time() - start, train_l_sum/batch_count, hamming, f1_macro, f1_micro, acc)\n",
    "        )\n",
    "        # if epoch == 30:\n",
    "        #     optimizer = torch.optim.Adam([Lambda], lr=0.002,betas=(0.9, 0.999), eps=1e-08)\n",
    "        # elif epoch == 50:\n",
    "        #     optimizer = torch.optim.Adam([Lambda], lr=0.001,betas=(0.9, 0.999), eps=1e-08)\n",
    "        # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    #     best_acc = 0.5791\n",
    "    #     temp = torch.tensor([-1.3607e-02, -5.3982e-02, -1.6628e-02,  2.5557e-02,  4.0150e-02,\n",
    "    #      1.1365e-01, -6.7485e-02, -4.5096e-02,  7.0721e-02,  3.9045e-02,\n",
    "    #      1.6875e-01, -1.0130e-01, -7.0357e-02,  1.7599e-01,  2.8182e-02,\n",
    "    #      8.5130e-03, -6.4480e-02, -1.5023e-02, -6.6069e-02, -3.4371e-03,\n",
    "    #     -1.4052e+01, -1.4002e+01, -1.4399e+01, -1.4142e+01, -1.4136e+01,\n",
    "    #     -1.3876e+01, -1.4346e+01, -1.4041e+01, -1.4566e+01, -1.3794e+01,\n",
    "    #     -1.4097e+01, -1.4222e+01, -1.4476e+01, -1.4082e+01, -1.4156e+01,\n",
    "    #     -1.3897e+01, -1.4380e+01, -1.4143e+01, -1.4241e+01, -1.3817e+01,\n",
    "    #     -1.4295e+01, -1.4292e+01, -1.4363e+01, -1.3645e+01],\n",
    "    #    requires_grad=True)\n",
    "    #     with torch.no_grad():\n",
    "    #         if acc <= best_acc:\n",
    "    #             Lambda.data.copy_(temp.data)\n",
    "    #         else:\n",
    "    #             best_acc = acc\n",
    "    #             temp.data.copy_(Lambda.data)\n",
    "    return Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Test_iter, test_target, Lambda, d, q, randLabels):\n",
    "    with torch.no_grad():\n",
    "        preLabels = []\n",
    "        for X in tqdm(Test_iter, total=len(Test_iter), ascii=True, desc=\"test\"):\n",
    "            preLabels.append(Pred(X[0][0], Lambda, d, q, randLabels))\n",
    "        preLabels = np.array(preLabels)\n",
    "        hamming = hamming_loss(test_target, preLabels)  # 汉明损失，越低越好\n",
    "        f1_macro = f1_score(test_target, preLabels, average=\"macro\")  # 0.6\n",
    "        f1_micro = f1_score(test_target, preLabels, average=\"micro\")\n",
    "\n",
    "        temp = preLabels == test_target\n",
    "        acc_list = []\n",
    "        for data in temp:\n",
    "            acc = 1\n",
    "            for x in data:\n",
    "                acc *= x\n",
    "            acc_list.append(acc)\n",
    "        acc = sum(acc_list) / len(acc_list)\n",
    "    return hamming, f1_macro, f1_micro, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pred(test_data, Lambda, d, q, randLabels):\n",
    "    bestLabels = None\n",
    "    z = Z(test_data, d, q, Lambda, device, randLabels)\n",
    "    bestP = -1.0\n",
    "    for i in range(len(randLabels)):\n",
    "        fk = f_k(test_data, randLabels[i], d, q, device)\n",
    "        temp_P = torch.exp((Lambda * fk).sum()) / z\n",
    "        if temp_P > bestP:\n",
    "            bestP = temp_P\n",
    "            bestLabels = randLabels[i]\n",
    "    return np.array(bestLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    dataSet=[0.5,0.1,0.3]\n",
    "    Labels=[1,0]\n",
    "    f=f_k(dataSet,Labels,3,2)\n",
    "    print(f)\n",
    "    \"\"\"\n",
    "# 训练集的处理\n",
    "train_data_path = \"./traindataReuters_all.npy\"\n",
    "train_label_path = \"./trainlabelReuters_all.npy\"\n",
    "test_data_path = \"./testdataReuters_all.npy\"\n",
    "test_label_path = \"./testlabelReuters_all.npy\"\n",
    "# train_data_path = \"./yeast_train_data.npy\"\n",
    "# train_label_path = \"./yeast_train_label.npy\"\n",
    "# test_data_path = \"./yeast_test_data.npy\"\n",
    "# test_label_path = \"./yeast_test_label.npy\"\n",
    "\n",
    "train_data = np.load(train_data_path)\n",
    "train_data = train_data[:, :]\n",
    "train_label = np.load(train_label_path)\n",
    "train_label = train_label[:, :]\n",
    "test_data = np.load(test_data_path)\n",
    "test_data = test_data[:, :]\n",
    "test_label = np.load(test_label_path)\n",
    "test_target = test_label[:, :]\n",
    "\n",
    "train_target = torch.tensor(train_label, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# 主成分降维\n",
    "pca = PCA(n_components=5)  # 保留5个主成分\n",
    "train_data = torch.tensor(pca.fit_transform(train_data), requires_grad=True)\n",
    "# train_data = torch.tensor(train_data, requires_grad=True)\n",
    "# print(train_data[0])\n",
    "\n",
    "d = len(train_data[0])\n",
    "q = len(train_target[0])\n",
    "K = int(d * q + 4 * comb(q, 2))\n",
    "thegma = 2 ** (1)  # 参数寻优，-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10497, 5])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10497, 4)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据集\n",
    "device = torch.device(\"cpu\")\n",
    "batch_size = 32\n",
    "dataset = Data.TensorDataset(train_data, train_target)\n",
    "# 测试集的处理\n",
    "\n",
    "test_data = Data.TensorDataset(torch.tensor((pca.fit_transform(test_data))))\n",
    "# test_data = Data.TensorDataset(torch.tensor(test_data, requires_grad=True))\n",
    "# 随机读取小批量\n",
    "train_iter = Data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_data, 1, shuffle=False)\n",
    "#\n",
    "# supported_rand_labels(train_label)\n",
    "# basic_rand_labels(q)\n",
    "mode = \"basic\"\n",
    "randLabels = generate_rand_Labels(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x2615917fd08>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   7%|6         | 23/329 [00:03<00:40,  7.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_3084/2014958150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     \u001b[0mrandLabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m )\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_3084/2097789798.py\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(objfunc, train_iter, test_iter, test_target, num_epochs, optimizer, thegma, Lambda, device, randLabels)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthegma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_3084/2109916996.py\u001b[0m in \u001b[0;36mobj_func\u001b[1;34m(DataSets, Labels, thegma, Lambda, randLabels, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mfk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataSets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataSets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mtemp_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_sum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLambda\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         temp_div = (\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_3084/3521504974.py\u001b[0m in \u001b[0;36mZ\u001b[1;34m(dataSet, d, q, Lambda, device, randLabels)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mfk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtemp_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLambda\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtemp_sum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_3084/1063027823.py\u001b[0m in \u001b[0;36mf_k\u001b[1;34m(dataSet, Labels, d, q, device)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 F_k.append(\n\u001b[0;32m     51\u001b[0m                     torch.tensor(\n\u001b[1;32m---> 52\u001b[1;33m                         \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                     )\n\u001b[0;32m     54\u001b[0m                 )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lambda = torch.tensor(\n",
    "#     np.random.rand(K), requires_grad=True, device=device, dtype=torch.float\n",
    "# )  # 初始点 \n",
    "# Lambda =torch.tensor([ 1.1060e-01,  1.2891e-01, -3.2866e-02,  1.1960e-01, -5.9192e-02,\n",
    "#         -6.9003e-02,  2.1254e-01, -3.6856e-02,  2.3860e-01,  1.4376e-01,\n",
    "#          3.2683e-01, -4.0302e-01,  9.6339e-02,  1.1465e-01,  1.3273e-01,\n",
    "#         -1.4790e-01,  6.8920e-02,  1.0594e-01,  2.8951e-02, -1.8953e-01,\n",
    "#          3.3756e-02,  1.8381e-02,  7.7972e-03,  1.3185e-01,  1.0120e-01,\n",
    "#          2.3428e-02, -1.6687e-01, -7.5685e-02,  2.3136e-03,  1.0234e-01,\n",
    "#          2.8742e-02,  5.6123e-02,  5.3597e-02, -4.2912e-03, -2.4144e-01,\n",
    "#         -4.1165e-02,  1.5462e-02, -5.8708e-02, -6.6897e-02,  1.7060e-01,\n",
    "#          4.9510e-02,  1.1143e-01,  1.4395e-01,  1.1102e-01,  8.3381e-02,\n",
    "#         -3.3308e-02,  1.5899e-01,  1.4644e-01, -7.0203e-02,  5.3325e-02,\n",
    "#          1.1100e-01,  1.5264e-03, -2.2287e-03, -5.9585e-02,  1.1739e-01,\n",
    "#         -1.2248e-01,  2.3899e-02, -3.8095e-02, -5.1161e-02, -1.2378e-01,\n",
    "#          6.5217e-02,  1.0395e-01, -2.1152e-01,  2.2577e-01, -6.4882e-03,\n",
    "#          7.8403e-02, -7.7026e-02,  2.0222e-01,  8.1410e-02,  7.4142e-02,\n",
    "#         -1.4224e-01,  3.6684e-02,  1.0633e-01,  9.9144e-02, -1.0876e-01,\n",
    "#          3.8941e-01,  1.0463e-01,  6.1642e-02,  5.3251e-02, -1.3368e-01,\n",
    "#          2.0914e-02, -1.7902e-02,  4.5028e-02, -1.3880e-02,  6.8724e-02,\n",
    "#          3.8901e-02, -5.4046e-02,  6.8841e-02,  1.2888e-01,  1.1804e-01,\n",
    "#         -1.6146e-01,  1.5285e-01,  8.5081e-02,  1.3078e-01,  5.3087e-03,\n",
    "#          2.2880e-01,  1.9101e-01,  1.9246e-01, -5.4693e-02,  1.1620e-02,\n",
    "#          8.0848e-02,  5.8956e-02, -9.5426e-02, -1.8407e-01,  1.5768e-01,\n",
    "#          1.6044e-01, -1.4881e-01, -9.7151e-02, -4.3380e-02, -3.3622e-03,\n",
    "#          5.6034e-02, -2.0823e-01,  1.0806e-01,  1.2957e-01,  3.2643e-02,\n",
    "#         -1.2202e-02,  1.1217e-01,  2.1918e-01,  5.8355e-06, -1.6637e-01,\n",
    "#          1.9173e-01,  1.0840e-01, -8.7356e-02, -1.1701e-01, -1.1002e-02,\n",
    "#          1.6890e-02,  1.4050e-01, -2.7408e-01,  1.1422e-02,  4.1725e-02,\n",
    "#         -1.2099e-01,  5.8066e-01, -5.6434e-03,  1.2136e-03, -3.4210e-02,\n",
    "#          6.8016e-01, -1.6435e-02, -4.4914e-02,  2.0760e-03,  4.6706e-01,\n",
    "#         -3.0609e-02, -5.2306e-02,  2.1807e-02, -5.3149e-02,  3.6104e-02,\n",
    "#          4.0167e-02,  1.7140e-01,  2.3728e-02,  4.4643e-02,  3.3928e-02,\n",
    "#         -1.1360e-02,  4.8301e-02,  8.4145e-03,  9.0057e-03,  9.4930e-02,\n",
    "#         -2.2399e-01, -3.7842e-03, -2.9024e-02,  2.6847e-02, -1.9178e-02,\n",
    "#         -3.1477e-02,  1.6695e-02, -2.4708e-04,  8.7243e-02,  1.1352e-01,\n",
    "#          7.5380e-02,  1.0744e-01,  1.1491e-01,  5.0565e-02,  2.8792e-02,\n",
    "#         -5.1602e-02, -2.5803e-01,  2.6961e-01,  2.8810e-01, -8.9398e-02,\n",
    "#          7.9644e-02,  2.3039e-01,  2.7814e-01, -9.8623e-02,  5.1343e-03,\n",
    "#          1.5740e-01,  1.7090e-01,  1.7396e-02, -1.3507e-01,  1.5925e-01,\n",
    "#          1.9609e-01,  3.8724e-02, -1.3261e-01,  1.2253e-01,  1.3421e-01,\n",
    "#          7.3776e-02,  1.2411e-01,  1.1991e-01,  1.2462e-01,  3.2155e-01,\n",
    "#          2.1812e-01,  1.2465e-01,  1.3106e-01,  1.4936e-01,  4.5185e-01,\n",
    "#          8.9912e-02,  4.1353e-02,  3.7427e-01,  2.0763e-01, -1.0323e-02,\n",
    "#          4.7101e-02,  1.5794e-01,  2.1201e-01,  2.6795e-02,  4.8086e-02,\n",
    "#          7.3452e-02,  1.5899e-01,  2.7094e-01,  1.9950e-01,  6.4753e-02,\n",
    "#         -8.4400e-02,  5.9152e-02,  2.3613e-03,  1.1030e-01, -6.7860e-02,\n",
    "#          2.5184e-03,  5.2043e-02, -6.2118e-02, -2.0245e-01, -1.6326e-02,\n",
    "#          2.6731e-02,  4.2211e-01,  6.3297e-01, -5.2351e-02, -2.8716e-02,\n",
    "#          2.7612e-01,  5.2174e-01,  9.7902e-02,  8.7654e-02,  1.0300e-01,\n",
    "#          7.2343e-03,  6.5939e-02,  1.3197e-01,  4.0094e-01, -4.1652e-02,\n",
    "#          5.3036e-02,  3.2880e-02,  4.8731e-01,  3.0857e-01,  4.0441e-02,\n",
    "#          4.8622e-02,  1.7460e-01,  5.2026e-01,  5.1831e-02, -4.6035e-02,\n",
    "#          4.8584e-01,  2.2022e-01,  2.0922e-02,  4.5327e-02,  1.7822e-01,\n",
    "#          4.1559e-01, -1.9413e-02,  3.0169e-04, -1.7722e-01, -7.8817e-02,\n",
    "#          1.4249e-01,  1.6098e-01, -3.4642e-01,  1.6393e-01,  7.2104e-02,\n",
    "#          6.4422e-02,  1.2601e-01, -2.8712e-01, -8.2173e-02, -3.0818e-02,\n",
    "#          2.9887e-01, -3.3247e-02,  5.9212e-02,  4.7733e-02,  2.9471e-01,\n",
    "#         -1.1061e-01, -5.1678e-02, -1.1294e-01,  1.5822e-01,  3.2727e-01,\n",
    "#          9.2444e-02,  8.8324e-02, -2.7437e-02,  2.3568e-01, -1.9502e-02,\n",
    "#          2.1329e-02,  2.9853e-02,  6.3013e-01, -4.4068e-02, -1.5697e-01,\n",
    "#         -1.1179e-02,  4.5506e-03,  4.6263e-02,  6.3008e-02,  2.9863e-02,\n",
    "#          5.4568e-02, -8.7568e-02, -8.3089e-02,  1.4185e-01, -6.3543e-02,\n",
    "#          1.0549e-01,  1.5917e-01,  3.0460e-01, -3.3359e-01,  2.0656e-01,\n",
    "#          1.9506e-01,  3.0794e-01, -4.1998e-01,  3.8008e-02,  4.1834e-02,\n",
    "#          2.6383e-01,  5.3604e-02,  1.3181e-01,  1.2294e-01,  4.3972e-01,\n",
    "#          1.1709e-01,  4.7870e-02,  2.2732e-02,  4.9303e-02, -4.7343e-02,\n",
    "#         -1.9305e-02,  4.8193e-02, -7.8233e-02,  4.7806e-02,  9.7495e-05,\n",
    "#         -5.0551e-02,  2.5404e-01, -5.3597e-02,  1.3268e-01,  2.2429e-01,\n",
    "#          2.2779e-02,  3.1488e-01,  3.3134e-03, -2.2103e-02,  3.0937e-01,\n",
    "#         -2.1294e-01,  1.5814e-02,  4.4241e-02,  1.8270e-01,  1.4547e-01,\n",
    "#          1.5248e-01,  5.4174e-02,  1.9122e-01,  2.2837e-01,  1.0581e-01,\n",
    "#          1.7039e-01,  3.0470e-01, -2.1793e-01, -2.6159e-02, -5.7892e-02,\n",
    "#          3.4461e-01, -5.9059e-01, -1.1255e-01, -3.2743e-02,  1.0925e-01,\n",
    "#         -7.4039e-02,  1.2495e-01,  1.6548e-01, -7.8420e-02,  3.3073e-01,\n",
    "#          9.3158e-02,  8.4230e-02, -9.2579e-02,  4.6507e-01,  5.4323e-02,\n",
    "#          1.0333e-01,  8.7364e-02,  6.1410e-01,  1.7659e-01,  2.2725e-01,\n",
    "#         -2.4356e-01,  1.3661e-01,  6.1566e-02,  5.9560e-02,  6.2763e-02,\n",
    "#          4.9676e-01,  1.6572e-01,  1.5559e-01, -2.5082e-01, -1.1748e-01,\n",
    "#          1.7051e-01,  1.1768e-01,  1.8481e-01, -4.3656e-01,  7.5439e-02,\n",
    "#          9.4334e-02,  3.3729e-02, -1.6214e-02,  4.4877e-03,  4.3966e-02,\n",
    "#         -2.2959e-02,  9.1131e-02,  1.0819e-01,  1.7791e-01,  1.3177e-01,\n",
    "#          1.7466e-01, -1.5815e-02, -2.5539e-02,  3.1724e-01, -2.1843e-01,\n",
    "#          1.0146e-01,  1.1613e-01,  1.8322e-01, -1.3279e-01,  1.1094e-01,\n",
    "#          1.2434e-01, -1.9698e-01,  1.8375e-01, -9.0627e-02, -7.5274e-02,\n",
    "#          1.4163e-01,  6.4854e-01, -2.9890e-02, -1.0156e+00, -1.0947e+00,\n",
    "#         -1.8915e-02, -1.0928e+00, -1.0490e+00, -1.3483e+00, -1.7844e+00,\n",
    "#         -8.5044e-01, -6.9808e-01, -1.0177e+00, -9.4430e-01, -7.1589e-01,\n",
    "#         -7.0497e-01, -1.0243e+00, -1.2275e+00, -1.1581e+00, -5.1944e-01,\n",
    "#         -1.2679e+00, -9.4034e-01, -8.7963e-01, -1.6425e+00, -8.5794e-01,\n",
    "#         -1.2049e+00], requires_grad=True)\n",
    "\n",
    "\n",
    "Lambda = torch.tensor([-1.1988e-02, -5.2650e-02, -1.8131e-02,  2.4598e-02,  3.9671e-02,\n",
    "         1.2995e-01, -6.7338e-02, -4.9823e-02,  7.0689e-02,  2.7791e-02,\n",
    "         1.7456e-01, -1.0386e-01, -6.9977e-02,  1.6798e-01,  2.6399e-02,\n",
    "         7.9110e-03, -6.1223e-02, -1.1949e-02, -6.6362e-02, -5.5660e-03,\n",
    "        -1.4056e+01, -1.3988e+01, -1.4395e+01, -1.4157e+01, -1.4139e+01,\n",
    "        -1.3862e+01, -1.4340e+01, -1.4056e+01, -1.4579e+01, -1.3779e+01,\n",
    "        -1.4088e+01, -1.4238e+01, -1.4488e+01, -1.4080e+01, -1.4152e+01,\n",
    "        -1.3899e+01, -1.4379e+01, -1.4144e+01, -1.4238e+01, -1.3818e+01,\n",
    "        -1.4294e+01, -1.4291e+01, -1.4360e+01, -1.3648e+01],\n",
    "       requires_grad=True)\n",
    "num_epochs = 10\n",
    "# optimizer = torch.optim.LBFGS([Lambda], lr=0.001)\n",
    "optimizer = torch.optim.Adam([Lambda], lr=0.001,betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "Lambda = Train(\n",
    "    obj_func,\n",
    "    train_iter,\n",
    "    test_iter,\n",
    "    test_target,\n",
    "    num_epochs,\n",
    "    optimizer,\n",
    "    thegma,\n",
    "    Lambda,\n",
    "    device,\n",
    "    randLabels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4219,  0.3179,  0.3100,  0.3262,  0.5201,  0.6068,  0.2907,  0.1044,\n",
      "         0.3724,  0.2115,  0.3200,  0.4062,  0.0607,  0.1945,  0.2309,  0.3505,\n",
      "         0.1622,  0.4449,  0.1262,  0.3358, -0.2075,  0.0766,  0.5853,  0.0017,\n",
      "         0.6564,  0.5548, -0.1624,  0.6698,  0.5878,  0.4539,  0.5941,  0.3025,\n",
      "         0.1586,  0.4095,  0.6774,  0.6503, -0.1377,  0.6092,  0.1168,  0.1416,\n",
      "         0.2743,  0.5307,  0.8794,  0.3395], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(Lambda)\n",
    "# [ 3.5124e-02, -8.5055e-02,  3.3453e-02, -1.7653e-02, -7.1441e-02,\n",
    "#         -1.5750e-01, -7.4084e-03,  1.1770e-01,  8.3856e-02,  2.6715e-01,\n",
    "#         -1.0405e-02,  4.3997e-02, -2.9593e-02,  3.3307e-01,  8.2613e-02,\n",
    "#         -2.1751e-02, -2.1640e-01, -1.2033e-01, -3.2102e-02,  8.6221e-02,\n",
    "#         -1.4092e+01, -1.4235e+01, -1.4270e+01, -1.3893e+01, -1.4207e+01,\n",
    "#         -1.4120e+01, -1.4363e+01, -1.3801e+01, -1.4291e+01, -1.4036e+01,\n",
    "#         -1.4195e+01, -1.3968e+01, -1.4245e+01, -1.4117e+01, -1.4324e+01,\n",
    "#         -1.3804e+01, -1.4239e+01, -1.4123e+01, -1.4247e+01, -1.3882e+01,\n",
    "#         -1.4161e+01, -1.4408e+01, -1.4324e+01, -1.3597e+01]\n",
    "# 目前最好的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|##########| 4500/4500 [00:23<00:00, 188.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming 0.459, f1_macro 0.434, f1_micro 0.565, subset acc 0.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hamming, f1_macro, f1_micro, acc = test(test_iter, test_target, Lambda, d, q, randLabels)\n",
    "print(\n",
    "    \"hamming %.3f, f1_macro %.3f, f1_micro %.3f, subset acc %.3f\"\n",
    "    % (hamming, f1_macro, f1_micro, acc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "hamming 0.209, f1_macro 0.155, f1_micro 0.208, subset acc 0.356,\n",
    "测试值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = torch.tensor(\n",
    "        [\n",
    "            1.3684e-03,\n",
    "            1.0080e-01,\n",
    "            2.1870e-03,\n",
    "            -1.9966e-02,\n",
    "            9.5491e-03,\n",
    "            -3.5565e-01,\n",
    "            5.6930e-02,\n",
    "            8.2445e-02,\n",
    "            -9.0433e-03,\n",
    "            7.7105e-02,\n",
    "            5.3024e-02,\n",
    "            -5.3038e-02,\n",
    "            -7.3672e-02,\n",
    "            1.1402e-01,\n",
    "            -1.4662e-01,\n",
    "            9.3514e-02,\n",
    "            -4.7597e-02,\n",
    "            -1.3051e-03,\n",
    "            2.2363e-02,\n",
    "            -2.6729e-02,\n",
    "            -1.4027e01,\n",
    "            -1.3943e01,\n",
    "            -1.4388e01,\n",
    "            -1.4196e01,\n",
    "            -1.4172e01,\n",
    "            -1.3829e01,\n",
    "            -1.4311e01,\n",
    "            -1.4092e01,\n",
    "            -1.4597e01,\n",
    "            -1.3747e01,\n",
    "            -1.4072e01,\n",
    "            -1.4278e01,\n",
    "            -1.4439e01,\n",
    "            -1.4082e01,\n",
    "            -1.4181e01,\n",
    "            -1.3875e01,\n",
    "            -1.4323e01,\n",
    "            -1.4137e01,\n",
    "            -1.4269e01,\n",
    "            -1.3813e01,\n",
    "            -1.4222e01,\n",
    "            -1.4327e01,\n",
    "            -1.4379e01,\n",
    "            -1.3603e01,\n",
    "        ],\n",
    "        requires_grad=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Lambda, \"./Lambda/Lambda.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ef38a575fa2916da2b52beed6bb11be645a5e972681097928f3a9c40a206c8b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
